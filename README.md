# TP Big Data : Kafka, HDFS et Spark

Ce TP permet de mettre en place un environnement Big Data complet avec Apache Kafka, Hadoop HDFS et Spark pour cr√©er un pipeline de donn√©es simple mais fonctionnel.

## Objectifs
- Configurer un cluster Hadoop avec Docker
- D√©marrer et utiliser Kafka pour la messagerie
- Produire et consommer des messages avec Kafka
- Stocker des donn√©es dans HDFS
- Analyser les donn√©es avec Spark

## Pr√©requis
- Docker install√© sur votre machine
- VS Code (ou autre √©diteur de code)
- Connaissances de base en Python

## √âtape 1: Configuration de l'environnement Docker

Cr√©ez un nouveau projet VS Code nomm√© TP1BigData, puis lancez les commandes suivantes dans le terminal:

```bash
# T√©l√©charger l'image Hadoop/Kafka
docker pull liliasfaxi/hadoop-cluster:latest

# Cr√©er un r√©seau d√©di√© pour les conteneurs
docker network create --driver=bridge hadoop

# Lancer le n≈ìud master
docker run -itd --net=hadoop -p 9870:9870 -p 8088:8088 -p 7077:7077 -p 16010:16010 -p 9092:9092 --name hadoop-master --hostname hadoop-master liliasfaxi/hadoop-cluster:latest

# Lancer les n≈ìuds workers
docker run -itd -p 8040:8042 --net=hadoop --name hadoop-worker1 --hostname hadoop-worker1 liliasfaxi/hadoop-cluster:latest
docker run -itd -p 8041:8042 --net=hadoop --name hadoop-worker2 --hostname hadoop-worker2 liliasfaxi/hadoop-cluster:latest
```

> **Note**: Ajouter le port 9092 au n≈ìud master pour exposer Kafka

![Configuration Docker](images/docker_config.png)
*Capture d'√©cran: Lancement des conteneurs Docker*

## √âtape 2: D√©marrage des services Hadoop et Kafka

Entrez dans le conteneur master et d√©marrez Hadoop:

```bash
# Entrer dans le conteneur master
docker exec -it hadoop-master bash

# D√©marrer Hadoop
./start-hadoop.sh

# V√©rifier les processus
jps
```

D√©marrer Kafka:

```bash
./start-kafka-zookeeper.sh

# V√©rifier les processus
jps
```

![V√©rification des processus](images/jps_output.png)
*Capture d'√©cran: R√©sultat de la commande jps montrant les processus Hadoop et Kafka*

## √âtape 3: Production de messages Kafka

Utilisez le fichier `produce.py` fourni dans le r√©pertoire pour produire une s√©rie de messages sur le topic data_stream.

Ex√©cutez le script:

```bash
# Installer confluent-kafka si n√©cessaire
pip install confluent-kafka

# Ex√©cuter le script
python produce.py
```

![Production de messages](images/produce_output.png)
*Capture d'√©cran: Production de messages Kafka*

## √âtape 4: Consommation de messages Kafka

Cr√©ez un script Python  pour lire les messages de Kafka:
Utilisez le fichier `final_consume_hdfs.py` pour consommer les messages produits par Kafka.


Ex√©cutez le script:

```bash
python final_consume_hdfs.py
```

![Consommation de messages](images/consume_output.png)
*Capture d'√©cran: Consommation de messages Kafka*

## √âtape 5: Stockage des donn√©es dans HDFS

Cr√©ez un r√©pertoire dans HDFS pour stocker les donn√©es:

```bash
hdfs dfs -mkdir -p /user/root/data
hdfs dfs -chmod -R 777 /user/root/data
```

Utilisez ensuite le fichier `consumeinhdfs.py` pour lire les messages Kafka et les √©crire automatiquement dans un fichier sur HDFS.

Ex√©cutez le script et produisez de nouveaux messages:

```bash
# Dans un terminal
python consumeinhdfs.py

# Dans un autre terminal
python produce.py
```

V√©rifiez que les donn√©es ont bien √©t√© √©crites dans HDFS:

```bash
hdfs dfs -cat /user/root/data/data_stream.txt
```

![Stockage HDFS](images/hdfs_output.png)
*Capture d'√©cran: Contenu du fichier dans HDFS*

## √âtape 6: Analyse des donn√©es avec Spark

Lancez le fichier `countmessagespark.py` pour lire les donn√©es HDFS avec Spark et effectuer une premi√®re analyse (nombre de messages, mots les plus fr√©quents, etc.).

Ex√©cutez le script:

```bash
python countmessagespark.py
```

![Analyse Spark](images/spark_output.png)
*Capture d'√©cran: R√©sultat de l'analyse Spark*

## R√©solution de probl√®mes courants

### Probl√®me: √âchec de connexion √† Kafka
- V√©rifiez que le port 9092 est bien expos√© dans Docker
- Utilisez `localhost:9092` √† l'int√©rieur du conteneur, mais l'adresse IP ou le nom d'h√¥te appropri√© depuis l'ext√©rieur

### Probl√®me: √âchec d'√©criture dans HDFS
- V√©rifiez les chemins HDFS (ils ne correspondent pas aux chemins du syst√®me de fichiers local)
- V√©rifiez les permissions des r√©pertoires dans HDFS
- Utilisez les commandes HDFS pour cr√©er des r√©pertoires et v√©rifier les permissions

### Probl√®me: Erreurs de taille avec Spark
Si vous rencontrez des erreurs de taille avec Spark, modifiez le fichier hdfs-site.xml:

```bash
nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```

Ajoutez cette configuration:

```xml
<property>
  <name>dfs.blocksize</name>
  <value>1m</value>
  <description>Block size for HDFS</description>
</property>
```

## üë• Contributeurs

| Nom | GitHub |
|-----|--------|
| üßë‚Äçüíª J.C Emmanuel Mopeno-Bia | [morningstar-47](https://github.com/morningstar-47) |
| üßë‚Äçüíª Mohamed Coulibaly | [momo](https://github.com/PseudoCollegue1) |
| üßë‚Äçüíª Coll√®gue 2 | [@PseudoCollegue2](https://github.com/PseudoCollegue2) |
| üßë‚Äçüíª Coll√®gue 3 | [@PseudoCollegue3](https://github.com/PseudoCollegue3) |


## Conclusion

Ce TP vous a permis de mettre en place un pipeline de donn√©es complet avec les technologies Big Data les plus courantes. Vous avez appris √† :
- Configurer et utiliser un cluster Hadoop via Docker
- Produire et consommer des messages avec Kafka
- Stocker des donn√©es dans HDFS
- Analyser des donn√©es avec Spark

Ces comp√©tences sont essentielles pour travailler sur des projets Big Data dans des environnements professionnels.